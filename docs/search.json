[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "My research",
    "section": "",
    "text": "I currently work full-time as an AI software engineer. This page recaps my work in statistics as a PhD. student at university of Geneva and as an instructor at Ecole Polytechnique Fédérale de Lausanne (EPFL) (from 2012 to 2020):"
  },
  {
    "objectID": "publications.html#a-deterministic-and-computable-bernstein-von-mises-theorem",
    "href": "publications.html#a-deterministic-and-computable-bernstein-von-mises-theorem",
    "title": "My research",
    "section": "A deterministic and computable Bernstein-von Mises theorem",
    "text": "A deterministic and computable Bernstein-von Mises theorem\nGuillaume Dehaene, 2019.\nArticle link.\nIn order to make Bayesian inference possible on large datasets, approximations are required. For example, computing the Laplace approximation is straightforward since it only requires finding the maximum of the posterior. However, while the Bernstein-von Mises theorem guarantees that the error of the Laplace approximation goes to in the limit of infinitely large datasets, it is hard to measure precisely the size of the error in a given example.\nThis article derives a tight and computable elegant approximation of the size of this error. I show that the Kullback-Leibler divergence between a given probability distribution and its Laplace approximation can be approximated using the “Kullback-Leibler variance”"
  },
  {
    "objectID": "publications.html#computing-the-quality-of-the-laplace-approximation",
    "href": "publications.html#computing-the-quality-of-the-laplace-approximation",
    "title": "My research",
    "section": "Computing the quality of the Laplace approximation",
    "text": "Computing the quality of the Laplace approximation\nGuillaume Dehaene, 2017, AABI NIPS 2017 Workshop.\nArticle link.\nBayesian inference requires approximations because the posterior distribution is generally uncomputable. The Laplace approximation is a fairly basic one which gives us a Gaussian approximation of the posterior distribution. This begs the question: how good is the approximation? The classical answer to this question is the Bernstein-von Mises theorem, which asserts that in the large-data limit, the Laplace approximation becomes exact. However, this theorem is mostly useless in practice, mostly because its assumptions are hard to check.\nThis article presents a computationally-relevant extension of the classical result: we give an explicit upper-bound for the distance between a given posterior and its Laplace approximation. The approach we follow can be extended to more advanced Gaussian approximation methods which we will do in further work."
  },
  {
    "objectID": "publications.html#expectation-propagation-in-the-large-data-limit",
    "href": "publications.html#expectation-propagation-in-the-large-data-limit",
    "title": "My research",
    "section": "Expectation Propagation in the large-data limit",
    "text": "Expectation Propagation in the large-data limit\nGuillaume Dehaene and Simon Barthelmé, 2017, Journal of the Royal Statistical Society, series B.\nArticle link.\nExpectation Propagation is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it. Our two main contributions consist in showing that EP is closely related to Newton’s method for finding the maximum of the posterior, and showing that EP is asymptotically exact, meaning that when the number of datapoints goes to infinity the method recovers the posterior exactly.\nWe also introduce some new theoretical tools that help analysing EP formally, including a simpler variant, called Average-EP (or Stochastic-EP), that is asymptotically equivalent to EP."
  },
  {
    "objectID": "publications.html#expectation-propagation-performs-a-smoothed-gradient-descent",
    "href": "publications.html#expectation-propagation-performs-a-smoothed-gradient-descent",
    "title": "My research",
    "section": "Expectation Propagation performs a smoothed gradient descent",
    "text": "Expectation Propagation performs a smoothed gradient descent\nGuillaume Dehaene, 2016, AABI NIPS 2016 Workshop.\nArticle link.\nNeurIPS AABI Workshop 2016 Disney Research Paper Awards\nIf one wants to compute a Gaussian approximation of a probability distribution, there are three popular alternatives: the Laplace approximation, the Gaussian Variational Approximation, and Expectation Propagation.\nI show in this work that the approximations found by these three methods are actually very closely related, as they all correspond to variants from the same algorithm. This shines a bright light on the deep connections between these three algorithms."
  },
  {
    "objectID": "publications.html#bounding-errors-of-expectation-propagation",
    "href": "publications.html#bounding-errors-of-expectation-propagation",
    "title": "My research",
    "section": "Bounding errors of expectation-propagation",
    "text": "Bounding errors of expectation-propagation\nGuillaume Dehaene and Simon Barthelmé, 2015, NIPS 2015.\nArticle link.\nExpectation Propagation (EP) is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it.\nOur contribution in this work consists in showing that, in the large-data limit, EP is asymptotically exact and, furthermore, more precise than the alternative Laplace approximation. However, our results only hold for strongly log-concave distributions, which very rarely exist."
  },
  {
    "objectID": "publications.html#the-problem-of-the-uncomputable-posteriors",
    "href": "publications.html#the-problem-of-the-uncomputable-posteriors",
    "title": "My research",
    "section": "The problem of the uncomputable posteriors",
    "text": "The problem of the uncomputable posteriors\nBayesian inference is a very interesting method for someone who is interested in an axiomatic approach to inference. Indeed, the statistician Cox set out a simple set of rules for a robot to represent, using real numbers, the strength of his beliefs in various propositions and proved that the only system which obeys these axioms is probability theory. Furthermore, the rule that the robot should use to update his beliefs when faced new information is Bayes’s rule. The only axiomatization of rational thinking about the world is thus Bayesian inference.\nHowever, there is a huge problem with Bayesian inference: in most cases, the computations required for exact implementation of the method are too expensive to be used in practice. Most of the practical research work on Bayesian methods actually revolves about how to deal with this thorny issue with various approximation schemes. These approximation schemes can be decomposed into two large families: sampling methods (dominated by Markov Chain Monte Carlo methods; acronym MCMC) which aim at producing samples from the posterior distribution and on which I don’t have much to say, and what I call approximate inference methods: methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution."
  },
  {
    "objectID": "publications.html#approximate-inference-schemes",
    "href": "publications.html#approximate-inference-schemes",
    "title": "My research",
    "section": "Approximate inference schemes",
    "text": "Approximate inference schemes\nMy work centers on methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution. These are often called “variational” methods but I’d rather call them approximate inference methods instead. This is because:\n\nthe word “variational” is already used for the Variational Bayes algorithm which, even though it is the most popular approximate inference method, is far from being the only one\n“variational” implies an optimisation, which means that the term variational excludes the Expectation Propagation algorithm\nthe only critic of the “approximate inference” vs “sampling” separation I have gotten is that sampling methods also aim at producing an approximation of the posterior. I still feel like this is fine, since sampling methods require quite a bit of further processing in order to answer questions about the posterior whereas approximate inference methods directy output an approximation.\n\nThere is currently a large number of open questions on such methods.\n\nThe most important one concerns the speed at which these algorithms perform their task. Most often, these algorithms perform an iteration until they reach a fixed-point. Estimating the number of loops needed for convergence is very important for being able to guarantee that our algorithms will run quickly.\nA second critical question concerns the quality of the approximation we obtain. We need to understand in which cases these algorithms are good enough, and in which cases we should use the more expensive but more accurate sampling methods. However, current theoretical results are inapplicable for a number of reasons: the hypotheses are untestable, they apply to approximations that are not in use, etc.\nThe final important question is of a more practical nature. It is simply whether the current versions of the algorithm we have are the best we can do, or whether there are better variants that are yet to be found. This can only be solved once we are able to compute the speed and the approximation-quality of current methods. We will then be able to see whether introducing slight changes to the current algorithms will improve them."
  },
  {
    "objectID": "publications.html#bayesian-statistics-for-the-frequentist",
    "href": "publications.html#bayesian-statistics-for-the-frequentist",
    "title": "My research",
    "section": "Bayesian statistics for the frequentist",
    "text": "Bayesian statistics for the frequentist\nAnother aspect of my work concerns trying to convince my frequentist colleagues that Bayesian inference is the best system of statistics.\nIn practice, the best way to do this is not to be a dogmatic Bayesian which goes on shouting about the Cox axioms and subjective probability, but instead to adopt the frequentist point of view on problems, and show that Bayesian methods are the best at solving these. Adopting Bayesian methods is then simply a matter of choosing the most efficient tool for solving problems.\nIn practice, following this idea means studying the posterior distribution as a function-valued random variable, and then studying the behavior of this random variable. My work expands on earlier work by, among other, Lecam on what is called the “Bernstein-von Mises theorem”. My objective is to expand current forms of this theorem so that they are:\nAs general as they can be. As efficient as they can be. Relevant in practical applications of Bayesian inference."
  },
  {
    "objectID": "posts/quarto_is_better.html",
    "href": "posts/quarto_is_better.html",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "",
    "text": "The sales pitch\nLatex has a very simple sales pitch.\n\nDo you want to author high-quality technical documents? Then we have the right tool for the job! (And it’s also mandatory if you plan on publishing it in an academic journal).\n\nPretty compelling!\nBut Latex is also 40 year old software that has a number of issues. What if I told you that you can better by using Quarto?\n\npublish to a much wider variety of formats including HTML.\ndynamic figures in HTML.\nsimplify the syntax of your source document.\nintegrate with Python, Javascript, Julia, R for figure generation (or any sort of content generation).\n\nEven more compelling, right?\n\n\nLatex use cases\nLet’s start by reviewing typical use cases for Latex. Here are some typical use cases:\n\nI want to author a technical document that I will distribute myself. E.g. exercise sheets, presentations, notes.\nI want to author a group of technical documents that I will distirbute myself. E.g. for an academic class, I would write class notes, multiple presentations, and exercise sheets.\nI want to author a research article that I will distribute myself.\nI want to author a research article to the specific requirements of the Journal of Awesome Research.\n\nAmong these requirements, point 4. is the most constrained. Many technical journals have integrated Latex as part of their publishing workflow. They thus require authors to submit their manuscripts as Latex files so that they can be processed automatically. For example, as part of the review process, the document is first compiled using review settings (increased margins, skipping lines, more space between paragraphs) whereas the final published article is compiled using more standard settings.\nEven if you aren’t forced into using Latex, it still provides a decent solution for all these points. Once you have mastered it, Latex produces beautiful and clear documents with consistent styling. It is slightly trickier to coordinate a corpus of documents but it is still doable.\nBut Latex is also more than 40 years old and like other long-running software, it has got a few limits. My main criticisms would be:\n\nheavy syntax. Obviously, with sufficient work, you reach a point where you can parse Latex (and having syntactic coloring helps!), but the ratio of code to content is never negligible.\npoor documentation. Latex is a confusing language and accessing the right level of detail is extremely hard. Most times, when you search for information about the right solution to a problem, you find an old stackoverflow post with a recipe with no explanation. Hopefully, that fixes your issue but your understanding doesn’t grow.\nvery steep learning curve. These first two points compound to make Latex very tricky to learn.\nslow compilation. Seeing the rendered result of what you are currently working on can take multiple seconds or even minutes.\nlimited output formats. Latex is built to generate pdf content, but HTML documents can provide a much better support:\n\nthey adapt to the screen size.\nthey support dynamic figures (zooming, selecting subsets of data, etc.).\nwith some degree of javascript mastery, the sky is the limit.\n\nhard to extend. It is hard to interact with the Latex core and integrate additional software with it.\n\nIf want to expand on point 5. a bit more, since it might seem like a fairly minor point. To me, a key aspect of evaluating software is seeing how it can integrate with other software: the value of software is also derived from the environment surrounding it. For example, if every math journal on earth used MSWord (shudders), then the value of Latex would be considerably diminished for mathematicians. The fact that it is not possible to easily integrate tweaks and improvements from other software into Latex is a mark against it. For example, if suddenly someone invents a great javascript visualization tool, then we lose value by not being able to use that easily in Latex.\nNow, please understand that these flaws do not mean that Latex is bad. It is a great system for typesetting documents. But we can do better, even if we want to remain compatible with the requirement of being able to produce a .tex file for journal submission. The solution is Quarto.\n\n\nQuarto improvements\nQuarto is a modern alternative to Latex. It is built for publishing technical documents, whether academic or not. Its key features are:\n\nclear and simple markdown syntax.\nbuilt-in support for generating dynamic content in Python, Javascript, Julia, or R.\nfocus on producing crisp HTML documents.\ndynamic figures.\nexpanded syntax for code blocks, diagrams, callouts, etc.\nLatex compatibility: Quarto documents can always be exported to Latex format.\nsupport for a huge variety of output formats including:\n\nMSWord and Powerpoint (and their open office variants), when you want to do manual tweaking.\nSeveral wiki formats.\nSeveral ebook formats.\n\nsupport for coordinated ensemble of documents: websites, books, etc. This website is written in Quarto.\neasily tweakable. Quarto has built-in support for extensions and a large library of user-supplied extensions.\n\nPersonally, the shift to markdown syntax is the biggest draw for me. The key idea of markdown is that simplicity is essential:\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. – John Gruber\n\nCompare the following two fairly minimal documents with a title and a list, in markdown and Latex:\n# Choosing a language\n\nWhen choosing a language to write content:\n\n1. Source document should be easily human readable.\n1. Source document should be easily computer readable, except where it conflicts with rule 1.\n\\section{Choosing a language}\n\nWhen choosing a language to write content:\n\n\\begin{enumerate}\n\\item Source document should be easily human readable.\n\\item Source document should be easily computer readable, except where it conflicts with rule 1.\n\\end{enumerate}\nNot quite convinced by this simple example? Then I leave as an exercise to you, dear reader, to imagine the Latex code that would generate these two blocks of raw code with syntactic coloring.\n\n\nTry Quarto\nCurious? Then give it a try and download Quarto!\nTry the following:\n\nget familiar with the basic syntax.\ntry mermaid diagrams.\nif you are already familiar with Python, try a plotly interactive figure\nrender to html and pdf, and compare the results.\nrender to Latex.\n\nAre you curious how Quarto manages to do all that? I’ll address that in a future post. Unlike Latex, it is very possible to understand what goes on under-the-hood of Quarto and tweak it."
  },
  {
    "objectID": "index.html#artificial-intelligence-engineer-1",
    "href": "index.html#artificial-intelligence-engineer-1",
    "title": "Guillaume Dehaene",
    "section": "Artificial Intelligence Engineer",
    "text": "Artificial Intelligence Engineer\n\nGeodaisics | Sept. 2023\n\nCreation and implementation of the internal validation procedures of AI models.\nCreation of a shared workflow and tooling for the research team.\nWrote and reviewed developpement procedures for the internal Quality Management System in accordance with ISO 13485 / 14971 / 62304 / …"
  },
  {
    "objectID": "index.html#senior-data-engineer",
    "href": "index.html#senior-data-engineer",
    "title": "Guillaume Dehaene",
    "section": "Senior data Engineer",
    "text": "Senior data Engineer\n\nMarelli - Smart Me Up | July 2022 - July 2023\n\nProject supervision: embedded gesture recognition (precision &gt;95%).\nDefinition of data needs for the Grenoble teams.\nResponsible for the internal online data annotation tool.\nSupervision of the annotation team (12 people). Creation and supervision of the data sharing procedures."
  },
  {
    "objectID": "index.html#rd-computer-vision-engineer",
    "href": "index.html#rd-computer-vision-engineer",
    "title": "Guillaume Dehaene",
    "section": "R&D computer vision engineer",
    "text": "R&D computer vision engineer\n\nMarelli Smart Me Up | April 2020 - June 2022\n\nCreation and implementation of an unsupervised stereo vision algorithm.\nCreation of an internal library to standardize R&D activity on neural networks.\nFeatures: automated code standards, unit testing, web visualization of results, reproducibility.\nTechnology watch on computer vision. Algorithms adapted: SwAV, Mean teacher, depth estimation, transformer."
  },
  {
    "objectID": "index.html#assistant-professor-in-statistics",
    "href": "index.html#assistant-professor-in-statistics",
    "title": "Guillaume Dehaene",
    "section": "Assistant professor in Statistics",
    "text": "Assistant professor in Statistics\n\nEcole Polytechnique Fédérale de Lausanne | Sept. 2016 - April 2020\n\nNeurips 2016 AABI workshop Disney Research Paper Awards awarded for: Expectation Propagation performs a smoothed gradient descent, G. Dehaene.\nCreation and implementation of a method to validate the results of a Bayesian statistical analysis\nSupervision of one PhD. and three master theses."
  },
  {
    "objectID": "index.html#computer-science",
    "href": "index.html#computer-science",
    "title": "Guillaume Dehaene",
    "section": "Computer science",
    "text": "Computer science\n\nPython (expert): django, pytorch, tensorflow\nRust\nJavascript\nHTML, CSS\ngit\nDocker\nLinux admin"
  },
  {
    "objectID": "index.html#management",
    "href": "index.html#management",
    "title": "Guillaume Dehaene",
    "section": "Management",
    "text": "Management\n\nAgile project management\nR&D supervision\nCI / CD"
  },
  {
    "objectID": "index.html#artificial-intelligence",
    "href": "index.html#artificial-intelligence",
    "title": "Guillaume Dehaene",
    "section": "Artificial Intelligence",
    "text": "Artificial Intelligence\n\nNeural networks\nComputer vision\nStatistical theory\nBayesian statistics"
  },
  {
    "objectID": "index.html#languages",
    "href": "index.html#languages",
    "title": "Guillaume Dehaene",
    "section": "Languages",
    "text": "Languages\n\nFrench (native)\nEnglish (Bilingual)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog. I write mostly about:\n\nstatistics, with a focus on Bayesian statistics, variational inference and theory.\ntypesetting documents with Quarto and alternatives.\nprogramming, mostly in Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t use Latex: Quarto is better!\n\n\n\n\n\n\ntypesetting\n\n\nquarto\n\n\nlatex\n\n\n\nLatex is the defacto standard typesetting tool in large fractions of the academic world, but can we do better? Now that Quarto is around, the answer is yes! \n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\ngithub pages setup for this website\n\n\n\n\n\n\nquarto\n\n\ngithub pages\n\n\n\nSetting up a Quarto website is not trivial. Here is my setup to publish a single github repository using the www.gandi.net DNS. \n\n\n\n\n\nMar 27, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/github_pages_setup_with_quarto.html",
    "href": "posts/github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "Setting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\nIn my case, I thought there would be a slight complication. I want to have multiple pages from multiple repositories:\n\none for my personal website (on which you are right now).\none for my other Quarto projects. For each one, I want to have an associated github pages which can serve as a demo or readme.\n\nMy initial plan was the following:\n\nI want the personnal website to use the root domain guillaumedehaene.com and the default subodmain www.guillaumedehaene.com while I want project pages to use another subdomain demo.guillaumedehaene.com. I would like project_a to live under: demo.guillaumedehaene.com/project_a, etc.\n\nThis didn’t work out at all, so I’ve brought all repositories together for now, a summarized in Figure 1.\n\n\n\n\n\n\nflowchart LR\n    subgraph github\n        direction TB\n        A\n        C\n        E\n    end\n    subgraph urls\n        direction TB\n        B\n        BB\n        D\n        DD\n        F\n        FF\n    end\n    A[Website repository] --&gt; B[guillaumedehaene.github.io]\n    B -- use URL --- BB[www.guillaumedehaene.com]\n    C[Project A] --&gt; D[guillaumedehaene.github.io/project_a] -- use URL --- DD[www.guillaumedehaene.com/project_a]\n    E[Project B] --&gt; F[guillaumedehaene.github.io/project_b] -- use URL --- FF[www.guillaumedehaene.com/project_b]\n\n\n\n\nFigure 1\n\n\n\n\n\nIn the future, it would be straightforward to pull out any demo into its own subdomain, but I’ll first test out the current setup and see if I can make it work. I’m a bit unhappy about the current demo having a different style than the rest of the website but I can live with it."
  },
  {
    "objectID": "posts/github_pages_setup_with_quarto.html#standard-deploy-to-github-pages",
    "href": "posts/github_pages_setup_with_quarto.html#standard-deploy-to-github-pages",
    "title": "github pages setup for this website",
    "section": "Standard deploy to github pages",
    "text": "Standard deploy to github pages\n\nQuarto configuration\nFirst, we need a Quarto website. I have decided that I will render the website on my machine, and that github will just serves the files once they are uploaded (corresponding to this section of the docs).\nThus, we just change the global website configuration so that quarto render compiles to the docs folder:\n\n\n_quarto.yml\n\nproject:\n    type: website\n    output-dir: docs\n\n\n\ngithub configuration\nNow, we need to setup github:\n\nadd a .nojekyll file at the root of repository.\ncreate a new github repository with the specific repository name USERNAME.github.io.\n\nnormally, github associates USERNAME.github.io/REPOSITORY_NAME to a given repository.\nif we use this special repository name, github uses the root URL USERNAME.github.io.\n\npush the project to the remote repository.\ngo to settings &gt;&gt; pages and tell github pages to publish from the docs folder on the main branch.\n\nAt this point, the page website guillaumedehaene.github.io is fully functional. Now we need to host it on its the custom domain."
  },
  {
    "objectID": "posts/github_pages_setup_with_quarto.html#custom-domain-for-github-page",
    "href": "posts/github_pages_setup_with_quarto.html#custom-domain-for-github-page",
    "title": "github pages setup for this website",
    "section": "Custom domain for github page",
    "text": "Custom domain for github page\n\nQuarto configuration\nWe will make it so that the website answers to my custom url www.guillaumedehaene.com. If we omit this step, the website will act as if the custom url just acts as a redirect to the pages url.\nFirst, we need to tell Quarto about the custom url:\n\ncreate a cname file at the project root containing the custom url:\n\n\ncname\n\nwww.guillaumedehaene.com\n\nmodify the project configuration file: _quarto.yml to include the cname file and so that Quarto uses the custom url:\n\n\n_quarto.yml\n\nproject:\n    # publishing to github pages\n    # ref: https://quarto.org/docs/publishing/github-pages.html#render-to-docs\n    type: website\n    output-dir: docs\n    resources:\n        - CNAME\n\nwebsite:\n    title: \"Guillaume Dehaene\"\n    site-url: www.guillaumedehaene.com\n\nrender and push to github.\n\n\n\nGandi DNS configuration\nWe will configure the DNS server so that it knows to serve the custom url to the right github pages url.\nOn gandi, I get to set the DNS configuration manually.\n@ 10800 IN SOA ns1.gandi.net. hostmaster.gandi.net. 1711531617 10800 3600 604800 10800\n@ 1800 IN A 185.199.108.153\n@ 1800 IN A 185.199.109.153\n@ 1800 IN A 185.199.110.153\n@ 1800 IN A 185.199.111.153\nwww 10800 IN CNAME guillaumedehaene.github.io.\ndemo 10800 IN CNAME guillaumedehaene.github.io.\nNB: the IP addresses are documented on this page of the github docs.\nI’m honestly a bit confused about exactly what is going on under the hood here, but as far as I can tell:\n\nthe first four lines set the binding between the root domain guillaumedehaene.com and my github page.\nthe fifth line sets the binding specifically for the www subdomain.\nthe sixth line sets the binding specifically for the demo subdomain. Initially, I wanted to use this to host the demo pages for my Quarto projects, but I’m not using it currently.\n\nThere’s an underlying magic step where github knows, because of the cname file that we need to add in each repository, which pages correspond to which subdomain:\n\nthe website cname file contains www.guillaumedehaene.com on websites that should use that domain. Currently, that is all my repositories.\nthe website cname file contains demo.guillaumedehaene.com on websites that should use that domain. I have tested that this works on a single demo repo. I’m not sure what would happen if I hooked up two. Github might be very confused if I do that 🤷.\n\n\n\ngithub configuration\nFinally, we tell github about the cname:\n\ngo to settings &gt;&gt; pages and tell github pages to use a Custom domain and specify your desired full domain.\nwait a day, then enable https.\n\nAs a security precaution, please also verify your domain with github pages."
  }
]