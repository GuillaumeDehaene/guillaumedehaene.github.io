[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "My research",
    "section": "",
    "text": "1 Publications\n  \n  1.1 A deterministic and computable Bernstein-von Mises theorem\n  1.2 Computing the quality of the Laplace approximation\n  1.3 Expectation Propagation in the large-data limit\n  1.4 Expectation Propagation performs a smoothed gradient descent\n  1.5 Bounding errors of expectation-propagation\n  \n  2 Research interests\n  \n  2.1 The problem of the uncomputable posteriors\n  2.2 Approximate inference schemes\n  2.3 Bayesian statistics for the frequentist\nI currently work full-time as an AI software engineer. This page recaps my work in statistics as a PhD. student at university of Geneva and as an instructor at Ecole Polytechnique Fédérale de Lausanne (EPFL) (from 2012 to 2020):"
  },
  {
    "objectID": "publications.html#publications",
    "href": "publications.html#publications",
    "title": "My research",
    "section": "1 Publications",
    "text": "1 Publications\n\n1.1 A deterministic and computable Bernstein-von Mises theorem\nGuillaume Dehaene, 2019.\nArticle link.\nIn order to make Bayesian inference possible on large datasets, approximations are required. For example, computing the Laplace approximation is straightforward since it only requires finding the maximum of the posterior. However, while the Bernstein-von Mises theorem guarantees that the error of the Laplace approximation goes to in the limit of infinitely large datasets, it is hard to measure precisely the size of the error in a given example.\nThis article derives a tight and computable elegant approximation of the size of this error. I show that the Kullback-Leibler divergence between a given probability distribution and its Laplace approximation can be approximated using the “Kullback-Leibler variance”\n\n\n1.2 Computing the quality of the Laplace approximation\nGuillaume Dehaene, 2017, AABI NIPS 2017 Workshop.\nArticle link.\nBayesian inference requires approximations because the posterior distribution is generally uncomputable. The Laplace approximation is a fairly basic one which gives us a Gaussian approximation of the posterior distribution. This begs the question: how good is the approximation? The classical answer to this question is the Bernstein-von Mises theorem, which asserts that in the large-data limit, the Laplace approximation becomes exact. However, this theorem is mostly useless in practice, mostly because its assumptions are hard to check.\nThis article presents a computationally-relevant extension of the classical result: we give an explicit upper-bound for the distance between a given posterior and its Laplace approximation. The approach we follow can be extended to more advanced Gaussian approximation methods which we will do in further work.\n\n\n1.3 Expectation Propagation in the large-data limit\nGuillaume Dehaene and Simon Barthelmé, 2017, Journal of the Royal Statistical Society, series B.\nArticle link.\nExpectation Propagation is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it. Our two main contributions consist in showing that EP is closely related to Newton’s method for finding the maximum of the posterior, and showing that EP is asymptotically exact, meaning that when the number of datapoints goes to infinity the method recovers the posterior exactly.\nWe also introduce some new theoretical tools that help analysing EP formally, including a simpler variant, called Average-EP (or Stochastic-EP), that is asymptotically equivalent to EP.\n\n\n1.4 Expectation Propagation performs a smoothed gradient descent\nGuillaume Dehaene, 2016, AABI NIPS 2016 Workshop.\nArticle link.\nNeurIPS AABI Workshop 2016 Disney Research Paper Awards\nIf one wants to compute a Gaussian approximation of a probability distribution, there are three popular alternatives: the Laplace approximation, the Gaussian Variational Approximation, and Expectation Propagation.\nI show in this work that the approximations found by these three methods are actually very closely related, as they all correspond to variants from the same algorithm. This shines a bright light on the deep connections between these three algorithms.\n\n\n1.5 Bounding errors of expectation-propagation\nGuillaume Dehaene and Simon Barthelmé, 2015, NIPS 2015.\nArticle link.\nExpectation Propagation (EP) is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it.\nOur contribution in this work consists in showing that, in the large-data limit, EP is asymptotically exact and, furthermore, more precise than the alternative Laplace approximation. However, our results only hold for strongly log-concave distributions, which very rarely exist."
  },
  {
    "objectID": "publications.html#research-interests",
    "href": "publications.html#research-interests",
    "title": "My research",
    "section": "2 Research interests",
    "text": "2 Research interests\nMy research interests are briefly summarized in this section.\n\n2.1 The problem of the uncomputable posteriors\nBayesian inference is a very interesting method for someone who is interested in an axiomatic approach to inference. Indeed, the statistician Cox set out a simple set of rules for a robot to represent, using real numbers, the strength of his beliefs in various propositions and proved that the only system which obeys these axioms is probability theory. Furthermore, the rule that the robot should use to update his beliefs when faced new information is Bayes’s rule. The only axiomatization of rational thinking about the world is thus Bayesian inference.\nHowever, there is a huge problem with Bayesian inference: in most cases, the computations required for exact implementation of the method are too expensive to be used in practice. Most of the practical research work on Bayesian methods actually revolves about how to deal with this thorny issue with various approximation schemes. These approximation schemes can be decomposed into two large families: sampling methods (dominated by Markov Chain Monte Carlo methods; acronym MCMC) which aim at producing samples from the posterior distribution and on which I don’t have much to say, and what I call approximate inference methods: methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution.\n\n\n2.2 Approximate inference schemes\nMy work centers on methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution. These are often called “variational” methods but I’d rather call them approximate inference methods instead. This is because:\n\nthe word “variational” is already used for the Variational Bayes algorithm which, even though it is the most popular approximate inference method, is far from being the only one\n“variational” implies an optimisation, which means that the term variational excludes the Expectation Propagation algorithm\nthe only critic of the “approximate inference” vs “sampling” separation I have gotten is that sampling methods also aim at producing an approximation of the posterior. I still feel like this is fine, since sampling methods require quite a bit of further processing in order to answer questions about the posterior whereas approximate inference methods directy output an approximation.\n\nThere is currently a large number of open questions on such methods.\n\nThe most important one concerns the speed at which these algorithms perform their task. Most often, these algorithms perform an iteration until they reach a fixed-point. Estimating the number of loops needed for convergence is very important for being able to guarantee that our algorithms will run quickly.\nA second critical question concerns the quality of the approximation we obtain. We need to understand in which cases these algorithms are good enough, and in which cases we should use the more expensive but more accurate sampling methods. However, current theoretical results are inapplicable for a number of reasons: the hypotheses are untestable, they apply to approximations that are not in use, etc.\nThe final important question is of a more practical nature. It is simply whether the current versions of the algorithm we have are the best we can do, or whether there are better variants that are yet to be found. This can only be solved once we are able to compute the speed and the approximation-quality of current methods. We will then be able to see whether introducing slight changes to the current algorithms will improve them.\n\n\n\n2.3 Bayesian statistics for the frequentist\nAnother aspect of my work concerns trying to convince my frequentist colleagues that Bayesian inference is the best system of statistics.\nIn practice, the best way to do this is not to be a dogmatic Bayesian which goes on shouting about the Cox axioms and subjective probability, but instead to adopt the frequentist point of view on problems, and show that Bayesian methods are the best at solving these. Adopting Bayesian methods is then simply a matter of choosing the most efficient tool for solving problems.\nIn practice, following this idea means studying the posterior distribution as a function-valued random variable, and then studying the behavior of this random variable. My work expands on earlier work by, among other, Lecam on what is called the “Bernstein-von Mises theorem”. My objective is to expand current forms of this theorem so that they are:\nAs general as they can be. As efficient as they can be. Relevant in practical applications of Bayesian inference."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html",
    "href": "posts/2024/05/statistics_experiment_aar.html",
    "title": "After action report: running a simple statistical experiment",
    "section": "",
    "text": "1 Experiments in statistics\n  2 The situation\n  \n  2.1 Data-generating model\n  2.2 Performance comparison\n  2.3 Experiments\n  \n  3 Key points\n  4 Python project: physical organization\n  5 Generating a Student mixture"
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#experiments-in-statistics",
    "href": "posts/2024/05/statistics_experiment_aar.html#experiments-in-statistics",
    "title": "After action report: running a simple statistical experiment",
    "section": "1 Experiments in statistics",
    "text": "1 Experiments in statistics\nThe premise of this post might seem slightly weird to some. Can we actually run experiments in Statistics? But, at the heart of statistics lies the following trio:\n\nA probabilistic model, which generates the data.\nAn analysis algorithm, which produces some result from the data.\nA good property1 that the result has.\n\nFor example, the empirical mean of the dataset is close to the true mean of the data-generating model.\nCritically, the algorithm needs to be tuned to the model. Each analysis algorithm typically applies to an ensemble of models which share some features. If an algorithm is applied to a model that does not have the right features, then the good property probably does not apply.\nTypically, we find the constraints that the model must respect through a sophisticated probabilistic analysis and we prove a theorem clearly establishing when the good property holds. But such theorems are often limited: they only show that the property holds, up to a small error. That’s where experiments also have role to play:\n\nThey can give concrete proof of the performance of an algorithm.\nThey can serve as a basis to build intuition.\n\nThat’s why, even if it is probably a bit heretical for a statistician, I am a big believer in high-quality statistical experiments2."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#the-situation",
    "href": "posts/2024/05/statistics_experiment_aar.html#the-situation",
    "title": "After action report: running a simple statistical experiment",
    "section": "2 The situation",
    "text": "2 The situation\nThe precise details of my situation are not particularly important: my process typically does not vary that much. In this case, I have a classification problem where I want to predict the correct class, and also give a confidence level associated to that prediction. We have a current baseline method, and I have an improvement in mind.\nI thus have a simple plan in mind:\n\nI will create a python project (see Section 4 if you want details).\nWhere I can implement both the existing method and a variant.\nWhere I can tune the data-generating model.\nWhere I can compute the performance of both methods.\n\nThis will give a simple tunable benchmark to check that, under a wide range of conditions, the new method is indeed better.\n\n2.1 Data-generating model\nFor my model, I have the following constraints:\n\nI want to generate pairs consisting of a class \\(C\\) and 2D features \\(X\\). Each class will have a different distribution for \\(X\\). I want to be able to tune the number of classes, but I will use 3 classes.\nI want a model that has an explicit density function for \\(X\\) given \\(C\\).\nI don’t want to use a simple model, such as a Gaussian, Gamma, etc.\nI want to be able to tune the model easily from an external configuration file.\n\nUsing a mixture distribution is the simplest way to accomplish this: the density of the mixture is the weighted sum of the densities of each component. Using student distributions instead of Gaussians gives dense tails to the density, and I like using models which produce outliers. Please see Section 5 for details.\nAll of this data-generation mechanism is supported by a small amount of reusable python code. This means that all of the steps are straightforward to reproduce or modify in the future.\n\n\n2.2 Performance comparison\nIn this experiment, I want to compare:\n\na standard method.\nan improvement.\n\nIn order to make this comparison quantitative, I need to define:\n\nan experimental setup where both methods can be applied simultaneously.\none (or multiple) measures of the performance of each method.\n\nFor this specific example, my problem is a problem of classification. I should thus measure:\n\nwhether the predictions of each method are correct or not.\nwhether the uncertainty estimates associated to each method are correct or not.\n\nFor point 1. there are many standard measures of the quality of a prediction. Let’s pick the precision and recall as good baselines. For point 2. we enter trickier territory. I took a solid hour before deciding to use the Brier score (L2 loss over the probabilities) and a custom measure of the whether the predicted probabilities of each class match the actual probabilities. Explaining the details of this choice is out-of-topic for this post.\nThe critical points here are:\n\nI have several quantitative measures of the performance of both methods.\nI took the time to think in detail about these measures of performance.\nSince I’m working with artifical data, I can compare the performance to the best possible performance: the one that an oracle that knows the data-generating model would reach.\n\nAgain, all of this I translate into concise and reusable python code.\n\n\n2.3 Experiments\nFinally, I can combine my data-generating mecanism and my measures of performance. This yields several experiments to compare my methods to one-another, and to the performance of an oracle with knowledge of the data-generating process.\nI can then:\n\nobserve how tweaking the parameters of the experiments modifies the measures of performance for both algorithms, and the gap between the two,\nplay-around with the difficulty of the class. Here, this corresponds to separating the classes more,\nplay around with class-imbalance,\nplay around with drift between the training dataset and the validation dataset, for example by modifying the proportion of each class in both sets.\n\nOverall, this gave me great confidence in the gains of the tweaked method, and a bit of additional understanding of its strengths."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#key-points",
    "href": "posts/2024/05/statistics_experiment_aar.html#key-points",
    "title": "After action report: running a simple statistical experiment",
    "section": "3 Key points",
    "text": "3 Key points\n\nStatistics / data-science is an applied discipline. Controlled experiments with toy-data can thus be a great way to improve our understanding of our methods.\nThis requires using non-trivial data. I believe that a mixture of student distributions is a good starting point. It is critical to avoid cases that are too easy since they might lack some critical features of realistic data, such as:\n\noutliers / exceptional datapoints,\nambiguity between classes,\nnon-linearity.\n\nComparison between methods needs to be quantitative. We should use, if possible, simple and well-established measures of performance. We should understand the statistical relevance of these measures. If possible, we should compare the performance of all methods to the performance of an oracle knowning the data-generating process, since this provides an upper-bound on the performance of any method.\nThis should be supported by high-quality code that is easy to tweak and to reuse. This code should be tested to avoid bugs.\n\nI am a great believer in this approach to statistics / data science, and I hope you can succesfully integrate it in your own work."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#sec-project-management",
    "href": "posts/2024/05/statistics_experiment_aar.html#sec-project-management",
    "title": "After action report: running a simple statistical experiment",
    "section": "4 Python project: physical organization",
    "text": "4 Python project: physical organization\n\nIn case you need guidance on this point, here is my default file-structure in a project.\nproject_root/\n├─ data/\n│  ├─ some_data.csv\n├─ scripts/\n│  ├─ experiment_1.py\n│  ├─ experiment_2.py\n│  ├─ experiment_3.py\n├─ src/\n│  ├─ library_name/\n│  │  ├─ __ini__.py\n│  │  ├─ file.py\n├─ test/\n│  ├─ test1.py\n├─ .gitignore\n├─ pyproject.toml\n├─ README.md\nIt’s a very standard structure:\n\nI’m using the modern pyproject.toml specification for dependencies and setup, instead of having a requirements.txt file.\ndata is saved in a separate folder, in a human-readable format. It gets commited as part of the project.\nany generalist function is saved in a python module / library saved under src/library_name.\nany script manipulating these functions to achieve a result (here: run an experiment) is saved under scripts.\ntests (using pytest) are separately saved under the tests folder. Always write test code."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#sec-generating-student-mixture",
    "href": "posts/2024/05/statistics_experiment_aar.html#sec-generating-student-mixture",
    "title": "After action report: running a simple statistical experiment",
    "section": "5 Generating a Student mixture",
    "text": "5 Generating a Student mixture\n\nTo generate the parameterization of the mixture, for each class:\n\nI define a list of centers, which roughly draw a tree-shape over the space. Each tree starts at 0,0, which will have an ambiguous class attribution. Each class goes in a different direction.\nI sample several (random number between 3 and 6) IID Gaussians centered around each center.\nI give each center a random weight, using a Gamma distribution.\n\nI tweaked the centers and the parameters of the sampling until I was visually satisfied with the result, then saved the mixture parameters in a json file. This makes sure that the mixture is (somewhat) human-readable but, more importantly, that it is machine-readable, can be shared trivially, can be resampled, can be commited to git, etc."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#footnotes",
    "href": "posts/2024/05/statistics_experiment_aar.html#footnotes",
    "title": "After action report: running a simple statistical experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI know that this is a very ambiguous statement. Data science has several frameworks which define the particular good properties that are interesting. For the present post, I do not wish to go further.↩︎\nNot convinced? Then consider deep neural networks. They are, by-far, the largest breakthrough in data science in the last 20 years, and they have been built purely on the back of empirical results.↩︎"
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html",
    "href": "posts/2024/04/aligning_dicom_pixels.html",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "",
    "text": "1 The problem\n  2 The solution\n  \n  2.1 A bit of geometry\n  2.2 Finding the matrices\n  \n  3 Bloopers\n  4 References"
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#the-problem",
    "href": "posts/2024/04/aligning_dicom_pixels.html#the-problem",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "1 The problem",
    "text": "1 The problem\nWe encountered the following situation at work. We had two files:\n\na DICOM file with a MRI scan of the subjects brain.\na NIFTI file with the segmentation of the brain into regions.\n\nThese are essentially 3D arrays of numbers, with a little bit of meta-data.\nThe issue is that the DICOM file and the NIFTI file can have arbitrary orientations with respect to one-another:\n\nthe axes might not be in the same order. For example, maybe we have:\ndicom.shape == 400, 300, 200\nnifti.shape == 300, 400, 200\nthe pixels of each axis might not be in the same order: maybe the first pixel in one format is the last pixel of the other format.\n\nThe problem is thus quite simple to state: there are 48 possibilities1. Let’s just open the docs, read the correct header keys, figure out how the arrays are organized, code the correct re-ordering and call it a day. Right? Right?\nWell, that’s certainly a plan. However, No plan survives first contact with the enemy2, and I can state with absolute certainty that the “documentation” of the DICOM and NIFTI standards (and ancillary software) is definitely hostile. Our initial hopes for a quick adventure were thus immediately blown to bits.\nStill, I pushed through and, after many bloopers, I have found the solution. I hope it can help somebody else in the future to tackle these f…antastic file formats."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#the-solution",
    "href": "posts/2024/04/aligning_dicom_pixels.html#the-solution",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "2 The solution",
    "text": "2 The solution\n\n2.1 A bit of geometry\nTo understand the solution, it is important to understand projective geometry. Do not sweat, we don’t need to understand everything3. We just need to know that, if we want to transform coordinates between two reference frames, then that can be reframed as a matrix multiplication.\n\n\n\n\n\n\nNote\n\n\n\n\nA change of reference frame from \\(R_1\\) to \\(R_2\\) can be represented by a \\(4,4\\) (or sometimes \\(3,3\\)) matrix \\(M_{R_2 \\leftarrow R_1}\\).\nComputing the inverse of the matrix gives the matrix for the opposite change of reference frame:\n\\[\n\\left(M_{R_2 \\leftarrow R_1}\\right)^{-1} = M_{R_1 \\leftarrow R_2}\n\\]\nComputing the matrix product \\(M_{R_3 \\leftarrow R_2} M_{R_2 \\leftarrow R_1}\\) gives the matrix for change of reference frame from \\(R_1\\) to \\(R_3\\).\n\n\n\nThat’s all we need to know, so feel free to skip to the next section: Section 2.2, or read my detailed explanations below.\nFor example:\n\nlet \\(x,y,z\\) denote the coordinates in the MRI room, with respect to the earth: \\(x\\) is the south-north axis, \\(y\\) is the west-east axis, \\(z\\) is the down-up axis. Let the origin be the middle of the door into the room.\nlet \\(a,b,c\\) denote the coordinates in the patient space. \\(a\\) is the left-right axis of the patient, \\(b\\) is the back-to-front axis, \\(c\\) is the feet-to-head axis. The origin point is the middle of the head of the patient.\n\nThen, there exists a matrix \\(M\\) of shape \\(4, 4\\) which can be used to translate from \\(x,y,z\\) coordinates to \\(a,b,c\\):\n\\[\n\\begin{pmatrix}\na \\\\ b \\\\ c \\\\ 1\n\\end{pmatrix}\n=\nM\n\\begin{pmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{pmatrix}\n\\]\nWait, why do we have constant coordinates \\(1\\) here? Why are the vectors 4D instead of 3? It’s needed so that we can also represent the change of origin using \\(M\\). If we are representing transformations between reference frames with the same origin, we can work with a \\(3, 3\\) matrix instead. However, I wanted to present the 4D case, because it is what is described in the DICOM and NIFTI docs.\nInverting the matrix reverses the direction of the change of variables:\n\\[\\begin{align}\n\\begin{pmatrix}\na \\\\ b \\\\ c \\\\ 1\n\\end{pmatrix}\n=\nM\n\\begin{pmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{pmatrix}\n\\\\\nM^{-1}\n\\begin{pmatrix}\na \\\\ b \\\\ c \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{pmatrix}\n\\end{align}\\]\nSimilary, if we had three frames of reference, then applying sequentially a change of reference from \\(R_1\\) to \\(R_2\\) then a second from \\(R_2\\) to \\(R_3\\) would give an overall change from \\(R_1\\) to \\(R_3\\). The same property holds for the associated matrices:\n\\[\nM_{R_3 \\leftarrow R_2} M_{R_2 \\leftarrow R_1} = M_{R_3 \\leftarrow R_1}\n\\]\n\n\n2.2 Finding the matrices\nSo now we know that we need to go looking for matrices.\nFor the NIFTI file format, this is immediate: the matrix is encoded as a header key4. If using nibabel, it is immediately accessible.\nCarefully reading the docs specifies that this affine matrix specifies the transformation between the voxel indices \\(i,j,k\\) and the RAS patient-space (the acronym gives the order and direction of the axes: Right then Anterior (i.e. back-to-front) then Superior (foot-to-head)).\nSurely, the dicom format must be similarly simple. Nope. However, since I know have digested the docs, here are the steps:\n\nFirst, there is a transposition between what the DICOM format calls voxels and how the array is organized on disk5. This is explained here.\nThen, the ImageOrientationPatient header specifies the first two columns of the matrix.\na = np.array(dicom.ImageOrientationPatient[:3])\nb = np.array(dicom.ImageOrientationPatient[3:])\n\n1matrix = np.zeros((4, 4))\nmatrix[3, 3] = 1\n\n2matrix[:3, 0] = b\nmatrix[:3, 0] = a\n\n1\n\nInitializing the matrix and specifying the fourth column.\n\n2\n\nNote the change of order with respect to ImageOrientationPatient. This is due to the transposition.\n\n\nFinally, by considering the change of position between two different dicom slices, we can find the third column.\nslice_diff = (np.array(dicom2.ImagePositionPatient) - np.array(dicom.ImagePositionPatient)) / (\n    dicom2.InstanceNumber - dicom.InstanceNumber\n)\n1c = slice_diff / np.sum(slice_diff**2) ** 0.5\n\n1\n\nNormalizing the vector so that it has norm 1.\n\n\n\nThis gives us the matrix to transform from the DICOM array coordinates \\(i,j,k\\) to the LPS patient-space. This is almost the same as the RAS space used by the NIFTI format: the first two axes are just pointing in the opposite direction.\nOverall, we are now able to combine:\n\nThe matrix from DICOM coordinates to LPS,\nThe matrix from LPS to RAS,\nThe matrix from NIFTI coordinates to RAS,\n\nin order to find the matrix corresponding to the change of variable we want:\n\\[\nM_{\\text{DICOM} \\leftarrow \\text{NIFTI}}\n=\n\\left[ M_{\\text{RAS} \\leftarrow \\text{LPS}} M_{\\text{LPS} \\leftarrow \\text{DICOM}} \\right]^{-1}\nM_{\\text{RAS} \\leftarrow \\text{NIFTI}}\n\\]\nAnd that’s it. We can now analyze the matrix to find how it swaps axes around and reorders them, and apply that transformation to the NIFTI array:\nmatrix_dicom_from_nifti = ...\nornt = nib.orientations.io_orientation(matrix_dicom_from_nifti)\nreoriented_nifti = nib.orientations.apply_orientation(nifti, ornt)\nHonestly, the only way you’ve made it this far is if you are yourself trying to deal with this exact problem. If so, then best of luck and bon courage. You will need both."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#bloopers",
    "href": "posts/2024/04/aligning_dicom_pixels.html#bloopers",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "3 Bloopers",
    "text": "3 Bloopers\nI can’t resist but tell you about all of the hilarious moments along the way were the DICOM spec blew up in my face.\n\nThe DICOM format does not define an ordering of DICOM slices. This means that different tools could choose different orderings. If you struggle with a mismatch of direction along the axis over which the slices are gathered, then the underlying issue might be that your tools do not align along this axis in the same way. NB: the difference in ordering in our case manifests in less than 5% of cases, so that was very tricky to debug.\nAfter a little bit of (unsuccesfully) poking around and trying to read the docs, I asked a colleague about the issue. We quickly went from: “Oh, but all dicoms have the same orientation.” to “No wait, there are two.” to “Well technically it’s maybe three.”. We then found a fourth one later in our tests.\nThe nibabel documentation, and the NIFTI docs I have found all repeat: “the NIFTI format uses a RAS reference frame” throughout. Imagine my surprise when I discovered the existence of the affine header.\nIn my first calculation of the shift between two DICOM slices, I initially naively thought that the slices would be in order and that the first file (with name slice_00000) would actually be the first slice. Ha. Ha. Ha. They are in a random order instead."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#references",
    "href": "posts/2024/04/aligning_dicom_pixels.html#references",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "4 References",
    "text": "4 References\n\nNibabel documentation on the orientation of dicom files.\nNibabel documentation on the transposition between voxels and the array coordinates.\nNibabel documentation: full formula for the DICOM affine matrix.\nNibabel documentation: the nifti affines.\nInnolitics DICOM documentation: Image Orientation Patient.\nInnolitics DICOM documentation: Image Position Patient."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#footnotes",
    "href": "posts/2024/04/aligning_dicom_pixels.html#footnotes",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(3!=6\\) possibilities for the order of the axes; \\(2^3=8\\) possibilities for the order of each axis.↩︎\nHistory of this quote↩︎\nSeriously, don’t worry if you don’t quite get it: this projective geometry stuff is a bit crazy.↩︎\nTechnically three but nibabel automatically chooses the most appropriate one.↩︎\nPossibly due to the differences between Fortran and C array layouts on disk?↩︎"
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html",
    "href": "posts/2024/03/quarto_website_configuration.html",
    "title": "Configuring a Quarto website",
    "section": "",
    "text": "1 Quarto website configuration\n  2 My choices\n  3 Page-specific options\n  4 Other interesting options"
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#quarto-website-configuration",
    "href": "posts/2024/03/quarto_website_configuration.html#quarto-website-configuration",
    "title": "Configuring a Quarto website",
    "section": "1 Quarto website configuration",
    "text": "1 Quarto website configuration\nQuarto offers a lot of configuration options for html and also for websites! In this post, I focus on the configuration options I believe to be the most important. These are, of course, a reflection of my personnal priorities, but I hope it can serve as a stepping stone towards your own mastery of Quarto.\nRemember that all of these options can be modified on a document-by-document basis by modifying the yaml header of the .qmd file."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#my-choices",
    "href": "posts/2024/03/quarto_website_configuration.html#my-choices",
    "title": "Configuring a Quarto website",
    "section": "2 My choices",
    "text": "2 My choices\nHere is my current global settings file with comments. Check out the latest version here.\n\n\n_quarto.yml\n\nproject:\n1    type: website\n    output-dir: docs\n    resources:\n        - CNAME\n\nwebsite:\n    title: \"Guillaume Dehaene\"\n    site-url: www.guillaumedehaene.com\n2    page-footer: \"This website was created with [Quarto](https://quarto.org/).\"\n3    page-navigation: true\n    back-to-top-navigation: true\n    navbar:\n        left:\n            - blog.qmd\n        right:\n            - publications.qmd\n            -   href: about.html\n                # file: about.qmd\n                text: About me\n\nformat:\n    html:\n4        theme:\n            - cosmo\n            - style.scss\n        mainfont: \"EB Garamond, Georgia, serif\"\n        monofont: Fira Code, consolas, courier, monospace\n        highlight-style: github\n\n        html-math-method:\n            method: mathjax\n            url: \"https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js\"\n        include-in-header:\n            text: |\n                &lt;style&gt;\n                @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap')\n                @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap')\n                &lt;/style&gt;\n                &lt;script&gt;\n5                MathJax = {\n                    tex: {\n                        tags: 'ams'  // should be 'ams', 'none', or 'all'\n                    },\n6                    output: {\n                        font: 'mathjax-fira'\n                    }\n                };\n                &lt;/script&gt;\n        \n7        toc: true\n        toc-location: right-body\n        \n8        number-sections: true\n        number-depth: 3\n\n9        shift-heading-level-by: 1\n        anchor-sections: true\n\n10        code-copy: true\n\n11        code-tools: true\n\n12        freeze: auto\n\n13        link-external-icon: true\n        link-external-newwindow: true\n\n14        lang: en\n\n15        strip-comments: true\n\n\n1\n\nPublishing to github pages using this method.\n\n2\n\nA simple footer which I’ll need to improve latter.\n\n3\n\nAdd page navigation information.\n\n4\n\nStyling options. See my blog post on how I built the styling.\n\n5\n\nNumber (almost all)1 all math equations.\n\n6\n\nStyling options. See my blog post on how I built the styling.\n\n7\n\nIncluding a toc menu:\n\n8\n\nNumbering sections, more or less like a Latex document. Headers from # to ### get numbered.\n\n9\n\nAdding an anchor symbol to headers. This is purely about communicating to the user that they can link to headers. shift-heading-level-by: 1 is necessary here. It converts # titles to &lt;h2&gt; instead of &lt;h1&gt;. &lt;h1&gt; elements do not receive the anchor treatment.\n\n10\n\nAdd a “copy code” anchor to code blocks. Another nice user-facing feature.\n\n11\n\nAdd a button to view the Quarto markdown source of each document. This is not a super useful feature but it has no downside.\n\n12\n\nAvoid repeating Python calculations to decrease document rendering time.\n\n13\n\nAdd additional styling to make obvious to the user which links are external and open these in new windows / tabs.\n\n14\n\nUse english language for automated language construction.\n\n15\n\nRemove html comments from the source. Any comment I write are about the Quarto content, and are not relevant for the HTML document."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#page-specific-options",
    "href": "posts/2024/03/quarto_website_configuration.html#page-specific-options",
    "title": "Configuring a Quarto website",
    "section": "3 Page-specific options",
    "text": "3 Page-specific options\nThese settings get applied globally accross the website, but they are not appropriate for special pages. Thankfully, we can override the global parameters on a given by giving them another value in the yaml header.\nI’ve used this feature to customize the settings on the about.html page.\nanchor-sections: false\nnumber-sections: false\ncode-tools: false\nThis page has a very different type of content. Numbering sections, anchoring all sections, and providing the page source are all features which distract from that content. Interestingly, some features are automatically removed just by using the about page format."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#other-interesting-options",
    "href": "posts/2024/03/quarto_website_configuration.html#other-interesting-options",
    "title": "Configuring a Quarto website",
    "section": "4 Other interesting options",
    "text": "4 Other interesting options\nThese options are presented in the order in which they appear in the Quarto docs:\n\nsmooth-scroll: instead of jumping to the target anchor, scroll the page smoothly. I feel that this improves the wow factor of the website, but it can also be annoying. It’s definitely a matter of personnal preference.\nhtml-math-method: choose the math renderer. mathjax is the best as far as I’m concerned (especially with version 4.0 on the way) but you can try out the other options.\nlinestretch: more space between lines. Can be the right choice if you want to produce a document that a reviewer would print. That’s not really a website option though.\nlightbox: Give a gallery scroller to the figures. I feel that this setting depends on the type of documents you write. For an article, I feel this should be on. I believe that Nature and several other journals use a similar type of styling for their figures.\ncrossref: if you want to customize the behavior of cross-references.\ninclude-in-header, include-before-body, include-after-body: these commands inject html code or files in specific sections of the final document. If you need to do some advanced features, you probably need this.\ncopyright, license: the information gets added to the appendix of the document."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#footnotes",
    "href": "posts/2024/03/quarto_website_configuration.html#footnotes",
    "title": "Configuring a Quarto website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEquations can either be delimited by $$ $$ signs or an ams-delimiter (for example: \\begin{equation} \\end{equation}). This numbers all ams equations. I’ll have more to say about math in future posts.↩︎"
  },
  {
    "objectID": "posts/2024/03/github_pages_setup_with_quarto.html",
    "href": "posts/2024/03/github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "1 Desired setup\n  2 Steps\n  \n  2.1 Standard deploy to github pages\n  2.2 Custom domain for github page"
  },
  {
    "objectID": "posts/2024/03/github_pages_setup_with_quarto.html#desired-setup",
    "href": "posts/2024/03/github_pages_setup_with_quarto.html#desired-setup",
    "title": "github pages setup for this website",
    "section": "1 Desired setup",
    "text": "1 Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\nIn my case, I thought there would be a slight complication. I want to have multiple pages from multiple repositories:\n\none for my personal website (on which you are right now).\none for my other Quarto projects. For each one, I want to have an associated github pages which can serve as a demo or readme.\n\nMy initial plan was the following:\n\nI want the personnal website to use the root domain guillaumedehaene.com and the default subodmain www.guillaumedehaene.com while I want project pages to use another subdomain demo.guillaumedehaene.com. I would like project_a to live under: demo.guillaumedehaene.com/project_a, etc.\n\nThis didn’t work out at all, so I’ve brought all repositories together for now, a summarized in Figure 1.\n\n\n\n\n\n\nflowchart LR\n    subgraph github\n        direction TB\n        A\n        C\n        E\n    end\n    subgraph urls\n        direction TB\n        B\n        BB\n        D\n        DD\n        F\n        FF\n    end\n    A[Website repository] --&gt; B[guillaumedehaene.github.io]\n    B -- use URL --- BB[www.guillaumedehaene.com]\n    C[Project A] --&gt; D[guillaumedehaene.github.io/project_a] -- use URL --- DD[www.guillaumedehaene.com/project_a]\n    E[Project B] --&gt; F[guillaumedehaene.github.io/project_b] -- use URL --- FF[www.guillaumedehaene.com/project_b]\n\n\n\n\nFigure 1\n\n\n\n\n\nIn the future, it would be straightforward to pull out any demo into its own subdomain, but I’ll first test out the current setup and see if I can make it work. I’m a bit unhappy about the current demo having a different style than the rest of the website but I can live with it."
  },
  {
    "objectID": "posts/2024/03/github_pages_setup_with_quarto.html#steps",
    "href": "posts/2024/03/github_pages_setup_with_quarto.html#steps",
    "title": "github pages setup for this website",
    "section": "2 Steps",
    "text": "2 Steps\n\n2.1 Standard deploy to github pages\n\nQuarto configuration\nFirst, we need a Quarto website. I have decided that I will render the website on my machine, and that github will just serves the files once they are uploaded (corresponding to this section of the docs).\nThus, we just change the global website configuration so that quarto render compiles to the docs folder:\n\n\n_quarto.yml\n\nproject:\n    type: website\n    output-dir: docs\n\n\n\ngithub configuration\nNow, we need to setup github:\n\nadd a .nojekyll file at the root of repository.\ncreate a new github repository with the specific repository name USERNAME.github.io.\n\nnormally, github associates USERNAME.github.io/REPOSITORY_NAME to a given repository.\nif we use this special repository name, github uses the root URL USERNAME.github.io.\n\npush the project to the remote repository.\ngo to settings &gt;&gt; pages and tell github pages to publish from the docs folder on the main branch.\n\nAt this point, the page website guillaumedehaene.github.io is fully functional. Now we need to host it on its the custom domain.\n\n\n\n2.2 Custom domain for github page\n\nQuarto configuration\nWe will make it so that the website answers to my custom url www.guillaumedehaene.com. If we omit this step, the website will act as if the custom url just acts as a redirect to the pages url.\nFirst, we need to tell Quarto about the custom url:\n\ncreate a cname file at the project root containing the custom url:\n\n\ncname\n\nwww.guillaumedehaene.com\n\nmodify the project configuration file: _quarto.yml to include the cname file and so that Quarto uses the custom url:\n\n\n_quarto.yml\n\nproject:\n    # publishing to github pages\n    # ref: https://quarto.org/docs/publishing/github-pages.html#render-to-docs\n    type: website\n    output-dir: docs\n    resources:\n        - CNAME\n\nwebsite:\n    title: \"Guillaume Dehaene\"\n    site-url: www.guillaumedehaene.com\n\nrender and push to github.\n\n\n\nGandi DNS configuration\nWe will configure the DNS server so that it knows to serve the custom url to the right github pages url.\nOn gandi, I get to set the DNS configuration manually.\n@ 10800 IN SOA ns1.gandi.net. hostmaster.gandi.net. 1711531617 10800 3600 604800 10800\n@ 1800 IN A 185.199.108.153\n@ 1800 IN A 185.199.109.153\n@ 1800 IN A 185.199.110.153\n@ 1800 IN A 185.199.111.153\nwww 10800 IN CNAME guillaumedehaene.github.io.\ndemo 10800 IN CNAME guillaumedehaene.github.io.\nNB: the IP addresses are documented on this page of the github docs.\nI’m honestly a bit confused about exactly what is going on under the hood here, but as far as I can tell:\n\nthe first four lines set the binding between the root domain guillaumedehaene.com and my github page.\nthe fifth line sets the binding specifically for the www subdomain.\nthe sixth line sets the binding specifically for the demo subdomain. Initially, I wanted to use this to host the demo pages for my Quarto projects, but I’m not using it currently.\n\nThere’s an underlying magic step where github knows, because of the cname file that we need to add in each repository, which pages correspond to which subdomain:\n\nthe website cname file contains www.guillaumedehaene.com on websites that should use that domain. Currently, that is all my repositories.\nthe website cname file contains demo.guillaumedehaene.com on websites that should use that domain. I have tested that this works on a single demo repo. I’m not sure what would happen if I hooked up two. Github might be very confused if I do that 🤷.\n\n\n\ngithub configuration\nFinally, we tell github about the cname:\n\ngo to settings &gt;&gt; pages and tell github pages to use a Custom domain and specify your desired full domain.\nwait a day, then enable https.\n\nAs a security precaution, please also verify your domain with github pages."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog. I write mostly about:\n\nstatistics, with a focus on Bayesian statistics, variational inference and theory.\ntypesetting documents with Quarto and alternatives.\nprogramming, mostly in Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter action report: running a simple statistical experiment\n\n\n\n\n\n\nstatistics\n\n\ndata science\n\n\n\nI wanted to test a simple statistical method so I ran a simulation-based experiment. This post gives an after action report of my priorities and process to validate my method. \n\n\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\nAligning a DICOM to a NIFTI, pixel-by-pixel\n\n\n\n\n\n\nrandom\n\n\n\nI have struggled at work to find how to align pixels. These pixels are particuarly vexing: they came from a DICOM file or a NIFTI file, which are both formats with awful documentation. If this helps a single person deal with this issue, then this post has reached its goal. \n\n\n\n\n\nApr 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nInstalling spotify’s annoy on a Windows machine\n\n\n\n\n\n\npython\n\n\n\nSpotify has shared the annoy library for nearest-neighbor calculations. However, trying to install it on Windows can give cryptic errors if python does not have access to the latest C compiler. In this post, I tell you how to fix this issue. \n\n\n\n\n\nApr 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nConfiguring a Quarto website\n\n\n\n\n\n\nquarto\n\n\n\nSetting up a Quarto website is not trivial. In this post, I discuss how to setup the _quarto.yml configuration file optimally. \n\n\n\n\n\nMar 31, 2024\n\n\n\n\n\n\n\n\n\n\n\nStyling a Quarto blog.\n\n\n\n\n\n\nquarto\n\n\ngithub pages\n\n\n\nCustomizing a website is fun and interesting. Let’s learn how to tweak the appearance of a Quarto website to make it our own. \n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n\n\n\n\n\nDon’t use Latex: Quarto is better!\n\n\n\n\n\n\ntypesetting\n\n\nquarto\n\n\nlatex\n\n\n\nLatex is the defacto standard typesetting tool in large fractions of the academic world, but can we do better? Now that Quarto is around, the answer is yes! \n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\ngithub pages setup for this website\n\n\n\n\n\n\nquarto\n\n\ngithub pages\n\n\n\nSetting up a Quarto website is not trivial. Here is my setup to publish a single github repository using the www.gandi.net DNS. \n\n\n\n\n\nMar 27, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "Guillaume Dehaene",
    "section": "Professional experience",
    "text": "Professional experience\n\nArtificial Intelligence Engineer\n\nGeodaisics | Sept. 2023\n\nCreation and implementation of the internal validation procedures of AI models.\nCreation of a shared workflow and tooling for the research team.\nWrote and reviewed developpement procedures for the internal Quality Management System in accordance with ISO 13485 / 14971 / 62304 / …\n\n\n\n\nSenior data Engineer\n\nMarelli - Smart Me Up | July 2022 - July 2023\n\nProject supervision: embedded gesture recognition (precision &gt;95%).\nDefinition of data needs for the Grenoble teams.\nResponsible for the internal online data annotation tool.\nSupervision of the annotation team (12 people). Creation and supervision of the data sharing procedures.\n\n\n\n\nR&D computer vision engineer\n\nMarelli Smart Me Up | April 2020 - June 2022\n\nCreation and implementation of an unsupervised stereo vision algorithm.\nCreation of an internal library to standardize R&D activity on neural networks.\nFeatures: automated code standards, unit testing, web visualization of results, reproducibility.\nTechnology watch on computer vision. Algorithms adapted: SwAV, Mean teacher, depth estimation, transformer.\n\n\n\n\nAssistant professor in Statistics\n\nEcole Polytechnique Fédérale de Lausanne | Sept. 2016 - April 2020\n\nNeurips 2016 AABI workshop Disney Research Paper Awards awarded for: Expectation Propagation performs a smoothed gradient descent, G. Dehaene.\nCreation and implementation of a method to validate the results of a Bayesian statistical analysis\nSupervision of one PhD. and three master theses."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Guillaume Dehaene",
    "section": "Skills",
    "text": "Skills\n\nComputer science\n\nPython (expert): django, pytorch, tensorflow\nRust\nJavascript\nHTML, CSS\ngit\nDocker\nLinux admin\n\n\n\nManagement\n\nAgile project management\nR&D supervision\nCI / CD\n\n\n\nArtificial Intelligence\n\nNeural networks\nComputer vision\nStatistical theory\nBayesian statistics\n\n\n\nLanguages\n\nFrench (native)\nEnglish (Bilingual)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Guillaume Dehaene",
    "section": "Education",
    "text": "Education\nPh.D. in neuroscience and statistics\nUniversité de Genève, Université Paris- Descartes\n2012-2016\nEcole Polytechnique engineer diploma\nEcole Polytechnique Paris\n2008-2012\nMaster in Cognitive Science\nENS-Paris, EHESS, Université Paris-Descartes\n2011-2012"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Guillaume Dehaene",
    "section": "Publications",
    "text": "Publications\nA deterministic and computable Bernstein-von Mises theorem\nG. Dehaene, 2019\nPresented at: Séminaire BIG (Grenoble), Séminaire de Statistique de Berne\nExpectation Propagation in the large data limit\nG. Dehaene and S. Barthelmé, 2018\nJournal of the Royal Statistical Society - Series B\nPresented at: Séminaire BIG (Grenoble), Séminaire de Statistique de Genève\nExpectation Propagation performs a smoothed gradient descent\nG. Dehaene, 2016\nAdvances in Approximate Bayesian Inference NeuRIPS workshop\nNeurIPS AABI Workshop 2016 Disney Research Paper Awards\nBounding errors of Expectation-Propagation\nG. Dehaene and S. Barthelmé, 2015\nNeurIPS 2015"
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Guillaume Dehaene",
    "section": "Hobbies",
    "text": "Hobbies\n\nJudo\nBandes dessinées\nCompetitive coding: Advent of code"
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html",
    "href": "posts/2024/03/quarto_is_better.html",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "",
    "text": "1 The sales pitch\n  2 Latex use cases\n  3 Quarto improvements\n  4 Try Quarto"
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#the-sales-pitch",
    "href": "posts/2024/03/quarto_is_better.html#the-sales-pitch",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "1 The sales pitch",
    "text": "1 The sales pitch\nLatex has a very simple sales pitch.\n\nDo you want to author high-quality technical documents? Then we have the right tool for the job! (And it’s also mandatory if you plan on publishing it in an academic journal).\n\nPretty compelling!\nBut Latex is also 40 year old software that has a number of issues. What if I told you that you can better by using Quarto?\n\npublish to a much wider variety of formats including HTML.\ndynamic figures in HTML.\nsimplify the syntax of your source document.\nintegrate with Python, Javascript, Julia, R for figure generation (or any sort of content generation).\n\nEven more compelling, right?"
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#latex-use-cases",
    "href": "posts/2024/03/quarto_is_better.html#latex-use-cases",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "2 Latex use cases",
    "text": "2 Latex use cases\nLet’s start by reviewing typical use cases for Latex. Here are some typical use cases:\n\nI want to author a technical document that I will distribute myself. E.g. exercise sheets, presentations, notes.\nI want to author a group of technical documents that I will distirbute myself. E.g. for an academic class, I would write class notes, multiple presentations, and exercise sheets.\nI want to author a research article that I will distribute myself.\nI want to author a research article to the specific requirements of the Journal of Awesome Research.\n\nAmong these requirements, point 4. is the most constrained. Many technical journals have integrated Latex as part of their publishing workflow. They thus require authors to submit their manuscripts as Latex files so that they can be processed automatically. For example, as part of the review process, the document is first compiled using review settings (increased margins, skipping lines, more space between paragraphs) whereas the final published article is compiled using more standard settings.\nEven if you aren’t forced into using Latex, it still provides a decent solution for all these points. Once you have mastered it, Latex produces beautiful and clear documents with consistent styling. It is slightly trickier to coordinate a corpus of documents but it is still doable.\nBut Latex is also more than 40 years old and like other long-running software, it has got a few limits. My main criticisms would be:\n\nheavy syntax. Obviously, with sufficient work, you reach a point where you can parse Latex (and having syntactic coloring helps!), but the ratio of code to content is never negligible.\npoor documentation. Latex is a confusing language and accessing the right level of detail is extremely hard. Most times, when you search for information about the right solution to a problem, you find an old stackoverflow post with a recipe with no explanation. Hopefully, that fixes your issue but your understanding doesn’t grow.\nvery steep learning curve. These first two points compound to make Latex very tricky to learn.\nslow compilation. Seeing the rendered result of what you are currently working on can take multiple seconds or even minutes.\nlimited output formats. Latex is built to generate pdf content, but HTML documents can provide a much better support:\n\nthey adapt to the screen size.\nthey support dynamic figures (zooming, selecting subsets of data, etc.).\nwith some degree of javascript mastery, the sky is the limit.\n\nhard to extend. It is hard to interact with the Latex core and integrate additional software with it.\n\nIf want to expand on point 5. a bit more, since it might seem like a fairly minor point. To me, a key aspect of evaluating software is seeing how it can integrate with other software: the value of software is also derived from the environment surrounding it. For example, if every math journal on earth used MSWord (shudders), then the value of Latex would be considerably diminished for mathematicians. The fact that it is not possible to easily integrate tweaks and improvements from other software into Latex is a mark against it. For example, if suddenly someone invents a great javascript visualization tool, then we lose value by not being able to use that easily in Latex.\nNow, please understand that these flaws do not mean that Latex is bad. It is a great system for typesetting documents. But we can do better, even if we want to remain compatible with the requirement of being able to produce a .tex file for journal submission. The solution is Quarto."
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#quarto-improvements",
    "href": "posts/2024/03/quarto_is_better.html#quarto-improvements",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "3 Quarto improvements",
    "text": "3 Quarto improvements\nQuarto is a modern alternative to Latex. It is built for publishing technical documents, whether academic or not. Its key features are:\n\nclear and simple markdown syntax.\nbuilt-in support for generating dynamic content in Python, Javascript, Julia, or R.\nfocus on producing crisp HTML documents.\ndynamic figures.\nexpanded syntax for code blocks, diagrams, callouts, etc.\nLatex compatibility: Quarto documents can always be exported to Latex format.\nsupport for a huge variety of output formats including:\n\nMSWord and Powerpoint (and their open office variants), when you want to do manual tweaking.\nSeveral wiki formats.\nSeveral ebook formats.\n\nsupport for coordinated ensemble of documents: websites, books, etc. This website is written in Quarto.\neasily tweakable. Quarto has built-in support for extensions and a large library of user-supplied extensions.\n\nPersonally, the shift to markdown syntax is the biggest draw for me. The key idea of markdown is that simplicity is essential:\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. – John Gruber\n\nCompare the following two fairly minimal documents with a title and a list, in markdown and Latex:\n# Choosing a language\n\nWhen choosing a language to write content:\n\n1. Source document should be easily human readable.\n1. Source document should be easily computer readable, except where it conflicts with rule 1.\n\\section{Choosing a language}\n\nWhen choosing a language to write content:\n\n\\begin{enumerate}\n\\item Source document should be easily human readable.\n\\item Source document should be easily computer readable, except where it conflicts with rule 1.\n\\end{enumerate}\nNot quite convinced by this simple example? Then I leave as an exercise to you, dear reader, to imagine the Latex code that would generate these two blocks of raw code with syntactic coloring."
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#try-quarto",
    "href": "posts/2024/03/quarto_is_better.html#try-quarto",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "4 Try Quarto",
    "text": "4 Try Quarto\nCurious? Then give it a try and download Quarto!\nTry the following:\n\nget familiar with the basic syntax.\ntry mermaid diagrams.\nif you are already familiar with Python, try a plotly interactive figure\nrender to html and pdf, and compare the results.\nrender to Latex.\n\nAre you curious how Quarto manages to do all that? I’ll address that in a future post. Unlike Latex, it is very possible to understand what goes on under-the-hood of Quarto and tweak it."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html",
    "href": "posts/2024/03/styling_a_quarto_blog.html",
    "title": "Styling a Quarto blog.",
    "section": "",
    "text": "1 Style objectives\n  2 Customization steps\n  \n  2.1 Summary\n  2.2 Overall style\n  2.3 Code blocks\n  2.4 Mermaid diagrams\n  2.5 Math blocks\n  2.6 Plotly figures\n  \n  3 Examples"
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#style-objectives",
    "href": "posts/2024/03/styling_a_quarto_blog.html#style-objectives",
    "title": "Styling a Quarto blog.",
    "section": "1 Style objectives",
    "text": "1 Style objectives\nWith any project, I like to start with a small list of objectives, instead of diving straight-away into code. This helps keep the project grounded.\nAs far as styling this website goes, I want to:\n\ncustomize the overall styling of the page.\ncustomize the font.\nmake sure the style is correctly applied to all special blocks:\n\ncode blocks.\nmermaid diagrams.\nMathJax math blocks.\nplotly figures.\n\n\nPlease refer to Section 3 for examples of how my current style is applied to various Quarto elements."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#customization-steps",
    "href": "posts/2024/03/styling_a_quarto_blog.html#customization-steps",
    "title": "Styling a Quarto blog.",
    "section": "2 Customization steps",
    "text": "2 Customization steps\n\n2.1 Summary\nI’m using:\n\nthe cosmo bootstrap theme. Once Quarto 1.5 is released, I’ll add a light-dark switch.\nEG Garamond as a text font, Fira Code for code blocks, mathjax-fira for math blocks.\nfor mermaid, I’m using the cosmo version of mermaid that Quarto introduces, but modified to have black text.\nfor code highlighting, I’m using the github style (I’m unsure if it actually matches what shows on github).\nI haven’t styled plotly plots. It is a bit complicated.\n\nMy current configuration is (or check out the latest version here):\n\n\n_quarto.yml\n\nformat:\n    html:\n        theme:\n            - cosmo\n            - style.scss\n        mainfont: \"EB Garamond, Georgia, serif\"\n        monofont: Fira Code, consolas, courier, monospace\n        highlight-style: github\n        \n        html-math-method:\n            method: mathjax\n1            url: \"https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js\"\n        include-in-header:\n            text: |\n2                &lt;style&gt;\n                @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap')\n                @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap')\n                &lt;/style&gt;\n                &lt;script&gt;\n                MathJax = {\n3                    tex: {\n                        tags: 'ams'  // should be 'ams', 'none', or 'all'\n                    },\n                    output: {\n                        font: 'mathjax-fira'\n                    }\n                };\n                &lt;/script&gt;\n\n\n1\n\nImporting Mathjax version 4.0 beta.\n\n2\n\nImporting the fonts.\n\n3\n\nUnrelated code: this numbers all equations.\n\n\nI also added an external style sheet: changing the font was causing a bad alignment in annotated code blocks, and I had to turn off a peculiar styling that Garamond applies to numerals. Check out the latest version here:\n\n\nstyle.scss\n\n/*-- scss:defaults --*/\nbody {\n    // By default, EG Garamond uses \"old-styles\" numerics which have a baseline offset\n    // for some numerals. This turns that ugly feature off.\n    font-variant-numeric: lining-nums;\n}\n\ndl.code-annotation-container-grid dt {\n    // needed because I have changed the mono font\n    line-height: 20px !important;\n}\n\n$mermaid-font-family: \"EB Garamond, Georgia, serif\";\n$mermaid-label-bg-color: #000000;\n$mermaid-label-fg-color: #000000;\n\n\n\n2.2 Overall style\nFor the overall style, I would like:\n\nsomething sleek and modern.\nsomething with a light and dark mode.\nsomething slightly personalized (but not too much! I’m really bad at graphical design).\nsomething simple.\n\nGiven that Quarto has built-in support for various free themes, I’ve decided to keep it simple and just use that.\n\nInitially, I used to combo of flatly + darkly but I dislike some of the blues they use.\nI’ve seen that, in version 1.5, Quarto will support light and dark mode styling for all themes. I’ve thus switched to using the cosmo theme. It is sleek, modern, and simple, like I wanted. Once the necessary update is pushed out, I will add light/dark mode and I will perhaps play around with the colors.\nto put a tiny bit of personalization, I’ve modified the text font to use Garamond instead (with appropriate fallbacks, just in case):\n\n\n_quarto.yml\n\nformat:\n    html:\n        mainfont: \"EB Garamond, Georgia, serif\"\n\nI’m unsure whether you need to add a call to download the font from the web in the HTML header: I’ve added one anyway.\nannoyingly, Garamond uses old-style numerals by default. This setting makes some numerals, such as 3 and 4, align the middle of the character to the baseline of the text. I’m honestly surprised that this even exists and I’ve turned it off.\n\n\nstyle.scss\n\n/*-- scss:defaults --*/\nbody {\n    // By default, EB Garamond uses \"old-styles\" numerics which have a baseline offset\n    // for some numerals. This turns that ugly feature off.\n    font-variant-numeric: lining-nums;\n}\n\n\n\n\n2.3 Code blocks\nFor code blocks, there are several basic choices available:\n\nthe font. Again, I’m going for a bit of personalization and using Fira code.\ncode highlighting.\n\nthe default setting is very grey: let’s try to have more color. 1\nafter testing most of them, my shortlist was:\n\ngithub\nsolarized\npygments\nbreeze\ngruvbox\n\nIn the end, I’ve decided on github. It’s light but detailed.\nIt would probably be worth it to explore a little bit more if I want something with more colors, like my setting on vscode.\n\n\nThe resulting website configuration is:\n\n\n_quarto.yml\n\nformat:\n    html:\n        monofont: Fira Code, consolas, courier, monospace\n        highlight-style: github\n\nChanging the font caused a bad alignment in annotated code blocks, so I had to introduce an external style. I must be doing something slightly wrong because I needed to add an !important tag to ensure that this took priority over the Quarto styling.\n\n\nstyle.scss\n\n/*-- scss:defaults --*/\ndl.code-annotation-container-grid dt {\n    // needed because I have changed the mono font\n    line-height: 20px !important;\n}\n\n\n\n2.4 Mermaid diagrams\nMermaid diagrams can either use:\n\nreactive styling, based on the base style (ie: cosmo for me): I didn’t find that it worked out great for cosmo since it uses a very luminous blue for text.\nusing the built-in themes. I didn’t think it worked great with the cosmo main theme.\ncustom styling. ⚠️ I struggled with this since it is not compatible with mermaid built-in themes ⚠️. I’ve just used this to set the font to black and the font-family to match the text font.\n\nI’ll probably want to return to this in the future, but for the time being, the diagrams are going to be neutral and easy on the eyes: good enough for now.\n\n\nstyle.scss\n\n$mermaid-font-family: \"EB Garamond, Georgia, serif\";\n$mermaid-label-bg-color: #000000;\n$mermaid-label-fg-color: #000000;\n\n\n\n2.5 Math blocks\nCurrently, the released version of MathJax: version 3.0, does not support much in the way of customization. Apparently, version 4.0 will allow to choose the font. Well, then let’s use the 4.0 beta version!\n\n\n_quarto.yml\n\nformat:\n    html:\n        html-math-method:\n            method: mathjax\n            url: \"https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js\"\n        include-in-header:\n            text: |\n                &lt;style&gt;\n                @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap')\n                @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap')\n                &lt;/style&gt;\n                &lt;script&gt;\n                MathJax = {\n                    tex: {\n                        tags: 'ams'  // should be 'ams', 'none', or 'all'\n                    },\n                    output: {\n                        font: 'mathjax-fira'\n                    }\n                };\n                &lt;/script&gt;\n\nI’ve set the font to mathjax-fira, to align with code blocks.\n\n\n2.6 Plotly figures\nAs far as I can tell, plotly does not support css-based customization of its figures. It is apparently possible to use existing or create new python templates to have consistent styling accross figures. Apparently, this dash-bootstrap-components library (pip link) includes templates that match the bootstrap styles that are built-into Quarto? It should even work with light-dark switching? I’ll make a note of it, and return to this if it becomes necessary."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#sec-examples",
    "href": "posts/2024/03/styling_a_quarto_blog.html#sec-examples",
    "title": "Styling a Quarto blog.",
    "section": "3 Examples",
    "text": "3 Examples\nSince this page is on the website, I’ll write down some examples, to have a visual reference.\nText formatting:\n\nitalics, bold, bold italics\nsuperscript2 / subscript2\nstrikethrough\n\n\nA blockquote\n\nA keyboard shortcut: Shift-Ctrl-PShift-Ctrl-P. 2\nA table:\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\nInline math \\(0+0=0\\) and inline code compute_something(foo). And their display equivalents:\n\\[\\begin{equation}\n0 + 0 = 0\n\\end{equation}\\]\n\\[\\begin{align}\n(a+2)(a-2) &= a^2 + 2a - 2a - 4 \\\\\n            &= a^2 - 4\n\\end{align}\\]\nThis block doesn't have syntax coloring.\nThe next one has python syntax coloring and a file title.\n\n\nsquare.py\n\ndef square(x: int) -&gt; int:\n    \"\"\"Compute the squared value.\n\n    This computes the square of the input.\n    \"\"\"\n    # This is high-level math!\n    return x**2\n\nA mermaid diagram, with figure styling:\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n This is a caption. \n\n\n\nA callout block:\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\nThis is a span that has the class aside which places it in the margin without a footnote number.\nThis is a plotly figure:\n\nimport plotly.express as px\nimport plotly.io as pio\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", \n                 color=\"species\", \n                 marginal_y=\"violin\", marginal_x=\"box\", \n                 trendline=\"ols\", template=\"simple_white\")\nfig.show()\n\n                                                \n\n\nAnnotated code block:\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#footnotes",
    "href": "posts/2024/03/styling_a_quarto_blog.html#footnotes",
    "title": "Styling a Quarto blog.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNB: the fact that the syntax highlighting is so grey could be due to the cosmo theme. Maybe the colors are computed from the base colors of the theme? That would be a weird choice.↩︎\nUnless I start doing IT posts, this probably won’t be useful, but there’s no reason to be lazy and not check it.↩︎"
  },
  {
    "objectID": "posts/2024/04/installing_spotify_annoy_windows.html",
    "href": "posts/2024/04/installing_spotify_annoy_windows.html",
    "title": "Installing spotify’s annoy on a Windows machine",
    "section": "",
    "text": "1 Installation errors for annoy\n  2 Installing the C compiler on Windows"
  },
  {
    "objectID": "posts/2024/04/installing_spotify_annoy_windows.html#installation-errors-for-annoy",
    "href": "posts/2024/04/installing_spotify_annoy_windows.html#installation-errors-for-annoy",
    "title": "Installing spotify’s annoy on a Windows machine",
    "section": "1 Installation errors for annoy",
    "text": "1 Installation errors for annoy\nWhen trying to install the annoy python library, you might encounter a cryptic error like: error: command 'gcc'. failed with exit status 1 This error is due to python not having access to the latest C compiler.\nWhen searching for a solution to this problem, I found many poor solutions which instead advised to use pre-compiled versions. Please, please, please never use random compiled code from unvetted third parties: that’s a major security risk! Instead, this error is very easy to fix: we just have to install the latest version of the C compiler."
  },
  {
    "objectID": "posts/2024/04/installing_spotify_annoy_windows.html#installing-the-c-compiler-on-windows",
    "href": "posts/2024/04/installing_spotify_annoy_windows.html#installing-the-c-compiler-on-windows",
    "title": "Installing spotify’s annoy on a Windows machine",
    "section": "2 Installing the C compiler on Windows",
    "text": "2 Installing the C compiler on Windows\nMicrosoft provides a download link for the C++ build tools. This actually downloads an installer util which will itself install the build tools:\n\nDownload the file from the Microsoft website.\nRun it.\nAmong the many installation options, select the latest version Visual Studio Build Tools (at the time of this writing, 2022).\n\n\nAnd done! You should now be able to install annoy:\npip install annoy\nWhen installing python packages, please keep in mind the good practice of creating a separate environment for each project. I recommend using the built-in venv util for this, and accessing it via the VSCode command line."
  }
]